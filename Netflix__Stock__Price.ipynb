{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KJkd5tzMNVg"
   },
   "source": [
    "# **Netflix Stock Price Prediction Using LSTM and XGBoost\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGdR-rNPSoYx"
   },
   "source": [
    "## **Business Problem**\n",
    "### **Data Types (Object Type for Numeric Columns)**\n",
    "**Problem:** All columns in the dataset, such as Open, High, Low, Close, Adj Close, and Volume, are of type object. These should likely be numeric (e.g., float or int), but they are stored as strings or mixed types.\n",
    "\n",
    "**Solution:** Convert the relevant columns (Open, High, Low, Close, Adj Close, and Volume) to their appropriate numeric data types (float for prices, int for volume). This will allow you to perform mathematical operations, such as calculations and analysis, on the data.\n",
    "\n",
    "### **Missing Values**\n",
    "**Problem:** There are missing values in some columns (High, Low, Close, Adj Close, Volume), as indicated by the non-null counts.\n",
    "\n",
    "The missing values could be caused by incomplete records or issues during data collection.\n",
    "\n",
    "**Solution:** Handle missing values either by filling them (e.g., with forward fill, backward fill, or interpolation) or by removing the rows that have missing values (if appropriate).\n",
    "\n",
    "### **Date Column Format**\n",
    "**Problem:** The Date column is stored as an object (likely a string) instead of a datetime object. This makes it harder to perform date-based filtering, sorting, or any time-based analysis.\n",
    "\n",
    "**Solution:** Convert the Date column to a datetime format to enable better time-based operate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71bvAmIzJP-F"
   },
   "source": [
    "## **Objectives**\n",
    "\n",
    "**Data cleaning:** Convert data types, handle missing values, and remove duplicates.\n",
    "\n",
    "**Exploratory Data Analysis:** Understand the data structure, trends, and relationships.\n",
    "\n",
    "**Time Series Analysis:** Explore and analyze trends, seasonality, and stationarity in stock prices and volumes.\n",
    "\n",
    "**Anomaly Detection:** Identify unusual price movements or trading volumes.\n",
    "\n",
    "**Data Imputation:** Address missing data using appropriate imputation techniques.\n",
    "\n",
    "**Stock Price Forecasting:** Build and evaluate models to predict future stock prices.\n",
    "\n",
    "**Volume Analysis:** Explore the relationship between stock prices and trading volumes.\n",
    "\n",
    "**Stock Split and Dividend Adjustments:** Investigate adjustments to Adj Close for stock splits and dividends.\n",
    "\n",
    "**Market Behavior Analysis:** Analyze market signals using technical indicators.\n",
    "\n",
    "**Report Generation:** Create visualizations and summaries to communicate findings effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pheHaMxAzP1"
   },
   "source": [
    "# **PART 1: DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eADe7RqTTQ_P"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oe1aBDkBaDX"
   },
   "source": [
    "## **Load and explore the dataset.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVe-GIOIBcr3"
   },
   "outputs": [],
   "source": [
    "# Loading the Netflix Stock Prices dataset\n",
    "df = pd.read_csv('Netflix Inc. (NFLX) Stock Price 2002-2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZL6VccmCJkA",
    "outputId": "59d7b256-5aff-46aa-bfac-e8685e628ab6"
   },
   "outputs": [],
   "source": [
    "#Checking the data size\n",
    "print(\"Dataset information\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEONwEcFaT1e"
   },
   "source": [
    "##### Based on the dataset description, it appears to be a financial time series dataset with 5729 entries. It contains typical stock market data columns (Date, Open, High, Low, Close, Adj Close, and Volume), but all columns are currently stored as 'object' data types (likely strings) rather than numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7yXWQ3SCTNs",
    "outputId": "8bf36f92-a4b3-4e0f-afb5-de0456eab064"
   },
   "outputs": [],
   "source": [
    "#Displaying the first 5 rows of the dataset\n",
    "print(\"\\nFirst 5 rows of the dataset\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8orrciWyacQh"
   },
   "source": [
    "##### Looking at this recent 5-day snapshot of stock data (Feb 19-25, 2025), there's a clear downward trend. The stock declined from $1,043.33 on Feb 19 to $977.24 on Feb 25, representing about a 6.3% drop. Trading volume increased in the final two days as the price fell, suggesting potentially higher selling pressure. Most concerning is that each day's close was lower than the previous day, forming a consistent bearish pattern across the entire period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjS11zBpCaTm",
    "outputId": "cab71256-c31e-428b-d33d-4023f21e7692"
   },
   "outputs": [],
   "source": [
    "#Displaying the Number of Columns in the dataset\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU0EyUnFVR74"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVNhKQ1CFozQ"
   },
   "source": [
    "##### The dataset has the following typical trading columns (Date, Open, High, Low, Close, Adj Close, Volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "PXZTfa16Ce-A",
    "outputId": "b2a66f59-26a2-43c6-e44f-072c578eb5b3"
   },
   "outputs": [],
   "source": [
    "#Checking if there is a Missing Value in the Dataset\n",
    "print(\"\\nMissing values in each column:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ_NF3I5at0X"
   },
   "source": [
    "##### The dataset has 2 missing values in each of the High, Low, Close, Adj Close, and Volume columns, while Date and Open columns have complete data. These missing values will need to be addressed before conducting time series analysis to avoid calculation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYi9vOrmCmlP"
   },
   "source": [
    "## **Handle missing values and perform data cleaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gADScgWgRBY"
   },
   "outputs": [],
   "source": [
    "# Remove trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwJeoZD-ClFD"
   },
   "outputs": [],
   "source": [
    "#Convert the relevant columns to numeric types\n",
    "columns_to_convert = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "df[columns_to_convert] = df[columns_to_convert].replace({',': ''}, regex=True)\n",
    "df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GCyvf8ggY8l"
   },
   "outputs": [],
   "source": [
    "# Fill missing values with forward fill\n",
    "df.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VGknfXmC0qz"
   },
   "outputs": [],
   "source": [
    "#convert the 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiJoXAqIC2sl",
    "outputId": "19813b69-283f-4948-9f6d-9fac95d0950c"
   },
   "outputs": [],
   "source": [
    "#Verify the data types after conversion\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfknmx8Wa7Oj"
   },
   "source": [
    "##### The data consists of numerical and datetime values, with columns such as \"Open,\" \"High,\" \"Low,\" \"Close,\" \"Adj Close,\" and \"Volume\" being of float64 type, indicating continuous numerical data. The \"Date\" column is of datetime64[ns] type, representing specific dates for each stock record. This structure allows for time-series analysis, tracking changes in stock prices and trading volume over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7aYDWfOC90h",
    "outputId": "2af0d75f-8329-435b-816a-61d4ed708cad"
   },
   "outputs": [],
   "source": [
    "#Reinspecting the shape of the data frame after filling in the missing values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M_jplW5amC_"
   },
   "source": [
    "##### The dataset has the following typical trading columns (Date, Open, High, Low, Close, Adj Close, Volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcRwGlPHDEI5",
    "outputId": "7d62c461-2cc7-4f08-d1e2-cfad272741bb"
   },
   "outputs": [],
   "source": [
    "# Check for missing values after cleaning\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkg4pMVKbDZv"
   },
   "source": [
    "##### After dropping all the missing values, the dataset has no missing values in each of the columns. All the columns have complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lH9AyYveDiZS",
    "outputId": "d1321c63-a4ad-43c1-a3aa-cc1bb1287d73"
   },
   "outputs": [],
   "source": [
    "#Function to generate boxplots for visuliazing outliers\n",
    "def check_outliers_boxplot(df, columns):\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Box Plot of {col}')\n",
    "        plt.show()\n",
    "# Generate boxplots for key columns\n",
    "check_outliers_boxplot(df,['Open','Close','Adj Close','Volume','High','Low'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp_7LBssV9QC"
   },
   "source": [
    "Based on the above there is outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_Ftwo_yD86O",
    "outputId": "94dd15c5-6569-410c-fe20-a18e078fb699"
   },
   "outputs": [],
   "source": [
    " #Handle outliers in volume\n",
    "Q1 = df['Volume'].quantile(0.25)\n",
    "Q3 = df['Volume'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter outliers\n",
    "df_filtered = df[(df['Volume'] >= lower_bound) & (df['Volume'] <= upper_bound)]\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "print(f\"DataFrame shape after removing outliers: {df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "5PAoiqELEDOK",
    "outputId": "19f418ae-baea-44c8-edd4-531571e583dc"
   },
   "outputs": [],
   "source": [
    "#Visualize Stock Price\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Closing Price')\n",
    "plt.title('Netflix Stock Price History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (Ksh)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZXul6qpbPfL"
   },
   "source": [
    "##### The graph depicts the historical closing price of Netflix (NFLX) stock from approximately 2002 to 2024.  Several key trends are apparent:\n",
    "\n",
    "*Early Stagnation*: For the first decade or so (2002-2012), the stock price remained relatively flat, fluctuating within a narrow range close to zero.\n",
    "*Gradual Growth*: Starting around 2012, the stock price began a steady, upward climb, indicating a period of consistent growth.\n",
    "*Rapid Acceleration*: The growth rate accelerated significantly after 2018, with the stock price experiencing a dramatic surge.\n",
    "*Recent Volatility*: The period from 2020 onwards shows increased volatility, with sharper peaks and troughs, suggesting more dramatic price swings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "JISx9GNREJg0",
    "outputId": "75102755-d897-4340-e680-c2b029779c84"
   },
   "outputs": [],
   "source": [
    "#Visualize Volume Analyis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Date'], df['Volume'], color='blue', alpha=0.7)\n",
    "plt.title('Trading Volume Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33QtH4QIbhGy"
   },
   "source": [
    "##### The chart shows trading volume over time, with notable trends and fluctuations:\n",
    "\n",
    "*Peak Trading Volume (2008-2015)*: The most significant spikes in trading volume occurred between 2008 and 2015, indicating heightened market activity. This could be due to major financial events, economic recovery periods, or increased investor interest.\n",
    "\n",
    "*Declining Volume Post-2015*: There is a noticeable decline in trading volume after 2015, suggesting reduced market activity. This may be due to decreased investor interest, market saturation, or regulatory changes affecting trading.\n",
    "\n",
    "*Volatility and Market Events*: The sharp peaks suggest periods of high volatility, possibly linked to economic crises, policy changes, or company-specific news that drove high trading activity.\n",
    "\n",
    "*Stable or Low Trading in Recent Years*: Since 2020, trading volume appears to be relatively low and stable, possibly indicating a matured market or lower investor engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atsz6sfdETur"
   },
   "source": [
    "## **Performing feature scaling and normalization where necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekFoqvOU9Keb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5i4m1B0FQWu",
    "outputId": "def676b4-63c7-4a44-aea6-2e32d5a44d23"
   },
   "outputs": [],
   "source": [
    "#Define columns to scale\n",
    "cols_to_scale = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "\n",
    "#MinMax scaling for LSTM\n",
    "minmax_scaler =MinMaxScaler(feature_range=(0,1))\n",
    "df_lstm = df.copy()\n",
    "df_lstm[cols_to_scale] = minmax_scaler.fit_transform(df_lstm[cols_to_scale])\n",
    "\n",
    "#Standard Scaling (for XGBoost)\n",
    "std_scaler = StandardScaler()\n",
    "df_xgb = df.copy()\n",
    "df_xgb[cols_to_scale] = std_scaler.fit_transform(df_xgb[cols_to_scale])\n",
    "\n",
    "# Check the scaled data\n",
    "print(\"LSTM Scaled Data Sample:\")\n",
    "print(df_lstm.head())\n",
    "\n",
    "print(\"\\nXGBoost Scaled Data Sample:\")\n",
    "print(df_xgb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc90y4FKbzzY"
   },
   "source": [
    "##### The data samples show stock market information for February 2025 with values scaled differently for two methods: LSTM scaling and XGBoost scaling.\n",
    "##### LSTM Scaled Data: The values are normalized between 0 and 1, making them suitable for deep learning models like LSTM. This scaling helps in capturing the relative changes in stock prices and volume, with a noticeable decrease in values from February 19 to February 25, suggesting a mild downward trend in the stock price.\n",
    "\n",
    "##### XGBoost Scaled Data: The values are scaled with a different approach, possibly through standardization or a method suited for tree-based or regression models like XGBoost. The scaled values are significantly higher, indicating that this data transformation method emphasizes the magnitude of stock price fluctuations, with negative values for the volume, likely indicating an inverse scaling or different reference point.\n",
    "\n",
    "##### Both datasets provide insights into price trends but are adjusted for different machine learning algorithms, reflecting how each method treats the scale and distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc1G21qSFxtr"
   },
   "source": [
    "# **PART 2 : FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lifshps4WSGI"
   },
   "source": [
    "## **Create new relevant features from the existing dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdIF-AP1FrDo",
    "outputId": "fe5b3335-06bf-4a86-9070-a18083c39495"
   },
   "outputs": [],
   "source": [
    "def create_rolling_statistics(df,columns,window_sizes=[5,10,20]):\n",
    "    df_rolling = df.copy()\n",
    "    for col in columns:\n",
    "        for window in window_sizes:\n",
    "            # Calculate moving averages\n",
    "            df_rolling[f'{col}_MA_{window}'] = df[col].rolling(window=window).mean()\n",
    "            # Calculate rolling standard deviation\n",
    "            df_rolling[f'{col}_STD_{window}'] = df[col].rolling(window=window).std()\n",
    "\n",
    "    # Drop NaN values resulting from the rolling operation\n",
    "    df_rolling = df_rolling.dropna()\n",
    "    return df_rolling\n",
    "\n",
    "# Define columns for which you want to create rolling statistics\n",
    "columns_to_rolling = ['Close', 'Open', 'High', 'Low', 'Volume']\n",
    "\n",
    "# Apply the function to create rolling statistics (MA and std)\n",
    "df_rolling = create_rolling_statistics(df, columns=columns_to_rolling, window_sizes=[5, 10, 20])\n",
    "\n",
    "# Display the shape of the dataset after adding rolling statistics\n",
    "print(f\"\\nDataset shape after adding rolling statistics: {df_rolling.shape}\")\n",
    "\n",
    "# Display the first few rows to inspect the new features\n",
    "print(df_rolling.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ag-u3IcCdg"
   },
   "source": [
    "##### The dataset, after feature engineering, contains 5,710 rows and 23 columns, with additional technical indicators like Price_Range, Daily_Return, Moving Averages (MA_5, EMA_5, etc.), Momentum, Volatility, RSI, and Bollinger Bands. These features enhance the dataset by providing deeper insights into stock price trends, volatility, and momentum, which are useful for predictive modeling or technical analysis. The data appears to be ready for machine learning or advanced analytics, with a focus on both price movements and market behavior over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic0uvGycGTrS"
   },
   "source": [
    "## **Utilize lagged variables for time series forecasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJHOBBTiGUam",
    "outputId": "bc6e2fbf-eb2d-4cfa-da45-8cff487a3c7a"
   },
   "outputs": [],
   "source": [
    "def create_lagged_features(df, columns, n_lags=5):\n",
    "    df_lagged = df.copy()\n",
    "    for col in columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "          df.loc[:, f'{col}_Lag_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "    # Drop the first n_lags rows as they will have NaN values due to shifting\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    return df_lagged\n",
    "\n",
    "# Define columns for which you want to create lagged features\n",
    "columns_to_lag = ['Close', 'Open', 'High', 'Low', 'Volume']\n",
    "\n",
    "# Apply the function to create lagged features\n",
    "df_lagged = create_lagged_features(df, columns=columns_to_lag, n_lags=5)\n",
    "\n",
    "# Display the shape of the dataset after adding lagged variables\n",
    "print(f\"Dataset shape after adding lagged features: {df_lagged.shape}\")\n",
    "\n",
    "# Display the first few rows to inspect the new features\n",
    "print(df_lagged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaYdFsiwcJSH"
   },
   "source": [
    "##### The dataset now contains 5,705 rows and 48 columns, with the addition of lagged features for stock prices and volume (e.g., Low_Lag_1, Volume_Lag_1, etc.). These lagged features capture the historical values of key indicators, providing insights into past market behavior and trends. This transformation is useful for time-series forecasting models, allowing them to leverage previous data points to predict future movements in stock prices and volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUBF7cPLJY4z"
   },
   "source": [
    "## **Implement rolling statistics such as moving averages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fyc-MjXgJZ1n",
    "outputId": "d37bddf8-dae1-4738-e2ca-5afc76f89954"
   },
   "outputs": [],
   "source": [
    "def create_rolling_statistics(df,columns,window_sizes=[5,10,20]):\n",
    "    df_rolling = df.copy()\n",
    "    for col in columns:\n",
    "        for window in window_sizes:\n",
    "            # Calculate moving averages\n",
    "            df_rolling[f'{col}_MA_{window}'] = df[col].rolling(window=window).mean()\n",
    "            # Calculate rolling standard deviation\n",
    "            df_rolling[f'{col}_STD_{window}'] = df[col].rolling(window=window).std()\n",
    "\n",
    "    # Drop NaN values resulting from the rolling operation\n",
    "    df_rolling = df_rolling.dropna()\n",
    "    return df_rolling\n",
    "\n",
    "# Define columns for which you want to create rolling statistics\n",
    "columns_to_rolling = ['Close', 'Open', 'High', 'Low', 'Volume']\n",
    "\n",
    "# Apply the function to create rolling statistics (MA and std)\n",
    "df_rolling = create_rolling_statistics(df, columns=columns_to_rolling, window_sizes=[5, 10, 20])\n",
    "\n",
    "# Display the shape of the dataset after adding rolling statistics\n",
    "print(f\"\\nDataset shape after adding rolling statistics: {df_rolling.shape}\")\n",
    "\n",
    "# Display the first few rows to inspect the new features\n",
    "print(df_rolling.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbL6dx3YcTMj"
   },
   "source": [
    "##### The dataset now has 5,691 rows and 78 columns, with the addition of rolling statistics like moving averages (e.g., MA_5, MA_10), standard deviations (e.g., Low_STD_10, Volume_STD_5), and other metrics such as Volume_MA_5 and Low_MA_20. These rolling statistics capture the trends and volatility over different time windows, enhancing the dataset for trend analysis and time-series forecasting. This transformation provides deeper insights into both price movements and trading volumes, helping models better understand market patterns over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JopMD9Wo2B1"
   },
   "source": [
    "# **PART 3: HYBRID APPROACH MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sveth-vaBCOP"
   },
   "source": [
    "## **Develop an LSTM model to capture temporal dependencies in stock prices.**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORVXpMxU95ve"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "LW3Q8mJUXjPc",
    "outputId": "6f2b4824-15e2-44f9-d947-31906de61fb2"
   },
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        # Use all columns except the first(date)\n",
    "        x = data[i:i + seq_length, 1:]\n",
    "        # Predict 'Close'\n",
    "        y = data[i + seq_length, 0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 60\n",
    "X, y = create_sequences(df_lstm[cols_to_scale].values, seq_length)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Build the LSTM Model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])), # Add Input layer\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100, return_sequences=False),\n",
    "    Dense(50),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and Train the Model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "#  Make Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Ensure predictions are reshaped correctly\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform using only the 'Close' feature\n",
    "predictions = minmax_scaler.inverse_transform(\n",
    "    np.hstack((predictions, np.zeros((predictions.shape[0], len(cols_to_scale) - 1))))\n",
    ")[:, 0]\n",
    "\n",
    "# Inverse transform the actual test labels\n",
    "y_test_original = minmax_scaler.inverse_transform(\n",
    "    np.hstack((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], len(cols_to_scale) - 1))))\n",
    ")[:, 0]\n",
    "\n",
    "# Visualize Predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_original, label='Actual Prices')\n",
    "plt.plot(predictions, label='Predicted Prices')\n",
    "plt.title('Netflix Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn1Hn1pgnebJ"
   },
   "source": [
    "##### The provided graph depicts a successful Netflix stock price prediction model, where the \"Predicted Prices\" (orange line) closely mirror the \"Actual Prices\" (blue line) across the entire time series. This strong alignment indicates that the model effectively captures the underlying trends and fluctuations in the stock's behavior, showcasing its accuracy and robustness. The accompanying low training, validation, and test loss values further support this observation, suggesting that the model has been trained effectively and generalizes well to unseen data. Essentially, the model demonstrates a high degree of precision in forecasting Netflix stock prices, with no significant deviations or anomalies, highlighting its potential for practical application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do1BT9-MB4Dx"
   },
   "source": [
    "## **XGBoost Model for comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "nZHT9JqSJd-u",
    "outputId": "93ab7f8b-0c2c-46e7-b2ce-400979ed19be"
   },
   "outputs": [],
   "source": [
    "#Number of previous days to use for prediction\n",
    "lag = 10\n",
    "\n",
    "# 'Close' price and create lagged features\n",
    "df_lagged = create_lagged_features(df[['Close']], columns=['Close'], n_lags=lag)\n",
    "\n",
    "# Split features and target\n",
    "X_xgb = df_lagged.drop(columns=['Close'])\n",
    "y_xgb = df_lagged['Close']\n",
    "\n",
    "# Train-test split\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Convert to DMatrix (optimized for XGBoost)\n",
    "dtrain = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "dtest = xgb.DMatrix(X_test_xgb, label=y_test_xgb)\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    # Root Mean Squared Error\n",
    "    'eval_metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 500,\n",
    "}\n",
    "\n",
    "# Train model with early stopping\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    #n_estimators\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtest, 'test')],\n",
    "    # added early stopping\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(dtest)\n",
    "\n",
    "# Calculate RMSE and R-squared\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test_xgb, y_pred_xgb)\n",
    "\n",
    "print(f'\\nXGBoost RMSE: {rmse_xgb:.4f}')\n",
    "print(f'\\nXGBoost R^2 Score: {r2_xgb:.4f}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_xgb.values, label='Actual Prices')\n",
    "plt.plot(y_pred_xgb, label='Predicted Prices (XGBoost)')\n",
    "plt.title('Netflix Stock Price Prediction - XGBoost')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHvzWyaJgR83"
   },
   "source": [
    "##### The XGBoost model, while capturing the general trend of Netflix's stock prices, demonstrates limitations in its predictive accuracy, particularly in the later time series where it produces flat, constant predictions. This is supported by a moderate RMSE of 0.9244 and an R^2 score of 0.5024, indicating that the model explains only about half of the variance in the actual price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgaSk67HCG29"
   },
   "source": [
    "## **Combine the models effectively for improved prediction accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xM5RzWK2-jjE",
    "outputId": "c5feef91-0a7d-429d-fd1b-5bbcff930ad8"
   },
   "outputs": [],
   "source": [
    "data = np.sin(np.linspace(0, 100, 1000))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:(i + time_step), 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 10\n",
    "X, y = create_dataset(data_scaled, time_step)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)  # LSTM input shape [samples, time steps, features]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Define and train the LSTM model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=False, input_shape=input_shape))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model((X_train.shape[1], 1))\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predict with LSTM\n",
    "lstm_predictions = lstm_model.predict(X_test)\n",
    "\n",
    "# Prepare data for XGBoost (reshape to 2D)\n",
    "X_reshaped = X.reshape(X.shape[0], X.shape[1])\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_reshaped, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, max_depth=3)\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Predict with XGBoost\n",
    "xgb_predictions = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Rescale the predictions back to the original scale\n",
    "lstm_predictions_rescaled = scaler.inverse_transform(lstm_predictions)\n",
    "xgb_predictions_rescaled = scaler.inverse_transform(xgb_predictions.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the RMSE for each model\n",
    "lstm_rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test.reshape(-1, 1)), lstm_predictions_rescaled))\n",
    "xgb_rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test_xgb.reshape(-1, 1)), xgb_predictions_rescaled))\n",
    "\n",
    "print(f\"LSTM RMSE: {lstm_rmse}\")\n",
    "print(f\"XGBoost RMSE: {xgb_rmse}\")\n",
    "\n",
    "# 1. **Weighted Average Ensemble**\n",
    "# We assign more weight to XGBoost because it performed better\n",
    "weight_lstm = 0.3\n",
    "weight_xgb = 0.7\n",
    "\n",
    "# Combine predictions\n",
    "ensemble_predictions = weight_lstm * lstm_predictions_rescaled + weight_xgb * xgb_predictions_rescaled\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test.reshape(-1, 1)), ensemble_predictions))\n",
    "print(f\"Ensemble RMSE (Weighted Average): {ensemble_rmse}\")\n",
    "\n",
    "# 2. **Stacking (Using Linear Regression as meta-model)**\n",
    "# Prepare the stacking dataset (use both LSTM and XGBoost predictions as features)\n",
    "stacking_X = np.column_stack((lstm_predictions_rescaled.flatten(), xgb_predictions_rescaled.flatten()))\n",
    "stacking_y = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacking_X, stacking_y)\n",
    "\n",
    "# Predict with meta-model\n",
    "stacking_predictions = meta_model.predict(stacking_X)\n",
    "\n",
    "# Evaluate stacking\n",
    "stacking_rmse = np.sqrt(mean_squared_error(stacking_y, stacking_predictions))\n",
    "print(f\"Stacking Model RMSE: {stacking_rmse}\")\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(scaler.inverse_transform(y_test.reshape(-1, 1)), color='blue', label='Actual')\n",
    "plt.plot(lstm_predictions_rescaled, color='red', label='LSTM Predictions')\n",
    "plt.plot(xgb_predictions_rescaled, color='green', label='XGBoost Predictions')\n",
    "plt.plot(ensemble_predictions, color='purple', label='Ensemble Predictions')\n",
    "plt.plot(stacking_predictions, color='orange', label='Stacking Model Predictions')\n",
    "plt.title('Model Comparison: LSTM, XGBoost, Ensemble, Stacking')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhMLG19Hgz4p"
   },
   "source": [
    "##### The graph reveals all models—LSTM, XGBoost, Ensemble, and Stacking—achieving remarkably high accuracy, visually demonstrating a near-perfect fit to the actual data. Quantitatively, XGBoost and the Stacking model exhibit the lowest RMSE, indicating superior predictive performance, with Stacking slightly edging out XGBoost. The Ensemble model also performs well, albeit with a slightly higher RMSE, while the LSTM, despite showing good learning through decreasing loss, yields comparatively lower accuracy. This suggests that for this specific dataset and prediction task, XGBoost and Stacking are the most effective, highlighting the potential benefits of meta-learning in stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pdDKxWh9iBo"
   },
   "source": [
    "# **PART 4: HYPERPARAMETER OPTIMIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeCfPWdZCTZd"
   },
   "source": [
    "## **LSTM Hyperparameter Optmization and Tuning**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7KluGWH8i2e"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcwhnAwW8Mis",
    "outputId": "934c42b7-acf0-4c90-b3b8-b0749fbc9241"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define model building function\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50),\n",
    "                   return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(units=hp.Int('units_2', min_value=50, max_value=200, step=50), return_sequences=False))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_lstm_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='lstm_tuning',\n",
    "    project_name='lstm_optimization'\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.1, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best LSTM units: {best_hps.get('units')}, Learning rate: {best_hps.get('learning_rate')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsUL8AWUYgsY"
   },
   "source": [
    "##### The model has completed Trial 10 with a very low validation loss (7.80e-08), indicating good performance. The best configuration so far is 150 LSTM units and a learning rate of 0.0001, with a total runtime of almost 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6PK2wbxD_6A",
    "outputId": "0b57c68c-16d5-42c5-88c8-06a1a65a8f5f"
   },
   "outputs": [],
   "source": [
    "# Build final LSTM model\n",
    "final_lstm_model = Sequential([\n",
    "    LSTM(200, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(200, return_sequences=False),\n",
    "    Dense(50),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile with best learning rate\n",
    "final_lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the final LSTM model\n",
    "final_lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjWV2jk8Y6LP"
   },
   "source": [
    "##### The model shows consistent improvement in training with a low loss, peaking at a loss of 0.00011379 in the final epoch. The validation loss fluctuates but reaches very low values, especially in Epochs 1, 5, and 14, where it drops to 1.36e-07 and 6.79e-08. This suggests the model is learning effectively, though some validation fluctuations remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpsgXoWrCnTa"
   },
   "source": [
    "\n",
    "\n",
    "## **XGBoost Hyperparameter Optmization and Tuning**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2eVW2vZSxAHU",
    "outputId": "2f9dd240-44d5-4896-b178-bcee6a16da8c"
   },
   "outputs": [],
   "source": [
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 300, 500]\n",
    "}\n",
    "\n",
    "# Create XGBoost regressor\n",
    "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Use GridSearchCV to find best parameters\n",
    "grid_search = GridSearchCV(xgb_regressor, param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)\n",
    "# Reshape X_train to be 2-dimensional\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "# Fit with reshaped data\n",
    "grid_search.fit(X_train_reshaped, y_train)\n",
    "# Best parameters\n",
    "best_xgb_params = grid_search.best_params_\n",
    "print(\"Best XGBoost Parameters:\", best_xgb_params)\n",
    "\n",
    "\n",
    "# Train XGBoost with best parameters\n",
    "best_xgb_model = xgb.XGBRegressor(**best_xgb_params)  # Unpack dictionary\n",
    "best_xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_xgb = best_xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Evaluate performance\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test_xgb, y_pred_xgb)\n",
    "\n",
    "print(f'Optimized XGBoost RMSE: {rmse_xgb:.4f}')\n",
    "print(f'Optimized XGBoost R² Score: {r2_xgb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo9r4HaPiozK"
   },
   "source": [
    "##### After hyperparameter tuning, the XGBoost model significantly improved, achieving a much lower RMSE (0.0029) and an almost perfect R² score (0.999). The optimized parameters—learning rate (0.05), max depth (3), and 500 estimators—helped the model generalize better and capture the underlying stock price patterns effectively. This highlights the importance of hyperparameter tuning in boosting model performance, transforming XGBoost from an underperforming model into a highly accurate predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrD7xLxmIVPq"
   },
   "source": [
    "#  **PART 5: MODEL  INTERPRETABILITY  AND EVALUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHRbO4zY_qq-"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "0ja-UUruFqSk",
    "outputId": "1b3e351d-e848-429b-e760-7835dac3f803"
   },
   "outputs": [],
   "source": [
    "feature_names = [f\"feature_{i}\" for i in range(X_test_xgb.shape[1])]\n",
    "feature_names = ['Close_Lag_1', 'Close_Lag_2', 'Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5',  'Open_Lag_1', 'Open_Lag_2', 'Open_Lag_3', 'Open_Lag_4', 'Open_Lag_5']\n",
    "\n",
    "X_test_xgb_df = pd.DataFrame(X_test_xgb, columns=feature_names)\n",
    "\n",
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(best_xgb_model)\n",
    "shap_values = explainer.shap_values(X_test_xgb_df)\n",
    "\n",
    "# Summary plot of feature importance\n",
    "shap.summary_plot(shap_values, X_test_xgb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPvlimZDjKhD"
   },
   "source": [
    "##### The SHAP summary plot provides insights into the impact of different lag features on the XGBoost model’s predictions. The Open_Lag_5 feature has the highest influence, with both high and low values contributing significantly to stock price predictions. Close_Lag_1 and Open_Lag_4 also show notable importance, indicating that recent closing and opening prices play a key role in forecasting. The SHAP values are centered around zero for most features, suggesting a balanced contribution without extreme effects. This analysis confirms that using historical stock prices improves predictive accuracy, with longer lags (especially 5-day lags) having a stronger impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "771ca0bb57ec46ae917ae2a416307af1",
      "aabf0605c7a94058ab4e8378fe9662d0",
      "732c43f123e944d79048ff27ffa2a154",
      "f093b6a156d34a8faace7ba4b38c5990",
      "91e76d19fca14a4b8a9a513aad55fd5c",
      "5fffef203ec547cbb5024742d6c2945b",
      "855f7b04e40c41b4858161727dab12ff",
      "a6ba956fa0464ba39fc69bd7368edd47",
      "036a8f21f1294b1b8217be896dae4ed4",
      "c30e7463edbd44b8ba4a726e5b699d5d",
      "f9df7d820aaa45638a642f626ad28a2c"
     ]
    },
    "id": "_IVjokoZNR5l",
    "outputId": "c8247ff4-13dc-46b2-e2ee-cd83bf2fa0d9"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Define a wrapper function for LSTM predictions\n",
    "def lstm_predict(X):\n",
    "    # Reshape back to 3D (time_steps, features) before passing to LSTM\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    return lstm_model.predict(X).flatten()\n",
    "\n",
    "# Select a subset of X_train for explanation\n",
    "X_sample = X_train[:10]\n",
    "\n",
    "# Reshape X_sample to 2D before passing to KernelExplainer\n",
    "X_sample_reshaped = X_sample.reshape((X_sample.shape[0], -1))\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer_lstm = shap.KernelExplainer(lstm_predict, X_sample_reshaped)\n",
    "\n",
    "# Select a subset of X_test for SHAP values\n",
    "X_test_for_shap = X_test[:50]\n",
    "\n",
    "# Reshape X_test to 2D\n",
    "X_test_for_shap_reshaped = X_test_for_shap.reshape((X_test_for_shap.shape[0], -1))\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values_lstm = explainer_lstm.shap_values(X_test_for_shap_reshaped)\n",
    "\n",
    "# Define feature names\n",
    "num_features = X_test_for_shap_reshaped.shape[1]\n",
    "feature_names = [f'Close_Lag{i+1}' if i < num_features // 2 else f'Open_Lag{i - num_features // 2 + 1}' for i in range(num_features)]\n",
    "\n",
    "# Summary plot to see feature importance\n",
    "shap.summary_plot(shap_values_lstm, X_test_for_shap_reshaped, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw6tN7V8j58N"
   },
   "source": [
    "##### The SHAP plot reveals that the most recent day's closing and opening prices (\"Close_Lag1\" and \"Open_Lag1\") are the most influential factors in predicting stock prices, highlighting the strong temporal dependencies within the data. Lagged opening and closing prices from further back exhibit progressively less impact, suggesting that short-term trends dominate the model's predictions. The distribution of feature values, indicated by color, demonstrates how specific price points affect predictions, with higher recent closing prices strongly correlating with increased predicted values. This analysis not only underscores the importance of recent data in time-series forecasting but also provides valuable insights for feature selection and model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6DgWAie0fNH"
   },
   "source": [
    "## **Evaluate model performance using metrics such as RMSE, MAE, and R-squared**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaMwYmpR_17_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5rIKzZP0IPL"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Evaluate LSTM Model\n",
    "lstm_pred_test = model.predict(X_test)\n",
    "lstm_rmse, lstm_mae, lstm_r2 = evaluate_model(y_test, lstm_pred_test)\n",
    "\n",
    "# Evaluate XGBoost Model\n",
    "xgb_pred_test = xgb_model.predict(X_test)\n",
    "xgb_rmse, xgb_mae, xgb_r2 = evaluate_model(y_test, xgb_pred_test)\n",
    "\n",
    "# Evaluate Hybrid Model\n",
    "hybrid_pred_test = hybrid_model.predict(X_test)\n",
    "hybrid_rmse, hybrid_mae, hybrid_r2 = evaluate_model(y_test, hybrid_pred_test)\n",
    "\n",
    "# Print results for comparison\n",
    "print(\"LSTM Model Evaluation:\")\n",
    "print(f\"RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, R^2: {lstm_r2:.4f}\\n\")\n",
    "\n",
    "print(\"XGBoost Model Evaluation:\")\n",
    "print(f\"RMSE: {xgb_rmse:.4f}, MAE: {xgb_mae:.4f}, R^2: {xgb_r2:.4f}\\n\")\n",
    "\n",
    "print(\"Hybrid Model Evaluation:\")\n",
    "print(f\"RMSE: {hybrid_rmse:.4f}, MAE: {hybrid_mae:.4f}, R^2: {hybrid_r2:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRDhcnqrOGz6",
    "outputId": "100fc7c5-71c9-4c36-9ba6-13d7b9612632"
   },
   "outputs": [],
   "source": [
    "# Predictions for LSTM\n",
    "y_pred_lstm = lstm_model.predict(X_test)\n",
    "y_pred_lstm = y_pred_lstm.flatten()\n",
    "\n",
    "# Predictions for XGBoost\n",
    "y_pred_xgb = best_xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"📊 {model_name} Performance:\")\n",
    "    print(f\"✅ RMSE: {rmse:.4f}\")\n",
    "    print(f\"✅ MAE: {mae:.4f}\")\n",
    "    print(f\"✅ R² Score: {r2:.4f}\\n\")\n",
    "\n",
    "# Evaluate LSTM\n",
    "evaluate_model(y_test, y_pred_lstm, \"LSTM\")\n",
    "\n",
    "# Evaluate XGBoost\n",
    "evaluate_model(y_test_xgb, y_pred_xgb, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZlHboSlkvfw"
   },
   "source": [
    "##### The performance metrics reveal a stark contrast between LSTM and XGBoost models, with XGBoost demonstrating significantly superior accuracy. XGBoost's exceptionally low RMSE and MAE values, coupled with a near-perfect R² score, indicate an almost flawless fit to the data, suggesting minimal prediction errors and exceptional explanatory power. While LSTM also exhibits strong performance, with a high R² score, its higher RMSE and MAE values suggest a less precise fit compared to XGBoost. This disparity underscores the effectiveness of XGBoost for this specific prediction task, raising the possibility of overfitting, especially with its near-perfect R² score. Therefore, rigorous validation with unseen data is crucial to ensure the model's generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC0B-Jpg0jUi"
   },
   "source": [
    "## **Compare the results of the hybrid model against standalone models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOAiHY5pPC7t",
    "outputId": "6cd91c8d-5ada-4cdb-8776-12b1c1b5662c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions from individual models\n",
    "lstm_preds = lstm_model.predict(X_test)\n",
    "xgb_preds = best_xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Reshape LSTM predictions to 1D array\n",
    "lstm_preds = lstm_preds.flatten()\n",
    "\n",
    "# Define weights for LSTM and XGBoost\n",
    "w_lstm = 0.5\n",
    "w_xgb = 0.5\n",
    "\n",
    "# Compute Hybrid Model Predictions\n",
    "hybrid_preds = (w_lstm * lstm_preds) + (w_xgb * xgb_preds)\n",
    "\n",
    "# Compute Metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n📊 {model_name} Performance:\")\n",
    "    print(f\"✅ RMSE: {rmse:.4f}\")\n",
    "    print(f\"✅ MAE: {mae:.4f}\")\n",
    "    print(f\"✅ R² Score: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Evaluate all models\n",
    "rmse_lstm, mae_lstm, r2_lstm = evaluate_model(y_test, lstm_preds, \"LSTM\")\n",
    "rmse_xgb, mae_xgb, r2_xgb = evaluate_model(y_test_xgb, xgb_preds, \"XGBoost\")\n",
    "rmse_hybrid, mae_hybrid, r2_hybrid = evaluate_model(y_test, hybrid_preds, \"Hybrid Model\") # Use y_test for hybrid\n",
    "\n",
    "# Compare Results\n",
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"LSTM\", \"XGBoost\", \"Hybrid\"],\n",
    "    \"RMSE\": [rmse_lstm, rmse_xgb, rmse_hybrid],\n",
    "    \"MAE\": [mae_lstm, mae_xgb, mae_hybrid],\n",
    "    \"R² Score\": [r2_lstm, r2_xgb, r2_hybrid]\n",
    "})\n",
    "\n",
    "print(\"\\n🔍 Model Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u94zOq2WlaML"
   },
   "source": [
    "##### The model comparison highlights that XGBoost outperforms both LSTM and the Hybrid model in terms of RMSE, MAE, and R² Score. XGBoost achieves the lowest RMSE (0.0029) and highest R² Score (0.9999), indicating exceptional accuracy in predicting stock prices. LSTM, while effective, shows higher errors (RMSE: 0.0246) and slightly lower predictive power (R²: 0.9952). The Hybrid model, which combines both approaches, improves upon LSTM’s performance (RMSE: 0.0125), but does not surpass XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYYCMZpZPl2S",
    "outputId": "763c21b6-8c75-484f-cf57-61b382d398c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Get predictions from LSTM and XGBoost\n",
    "lstm_train_preds = lstm_model.predict(X_train)\n",
    "lstm_test_preds = lstm_model.predict(X_test)\n",
    "\n",
    "xgb_train_preds = best_xgb_model.predict(X_train_xgb)\n",
    "xgb_test_preds = best_xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Stack predictions as new features\n",
    "stacked_train = np.column_stack((lstm_train_preds, xgb_train_preds))\n",
    "stacked_test = np.column_stack((lstm_test_preds, xgb_test_preds))\n",
    "\n",
    "# Train a meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "# Get final stacked predictions\n",
    "stacked_preds = meta_model.predict(stacked_test)\n",
    "\n",
    "# Evaluate stacked model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, stacked_preds))\n",
    "mae = mean_absolute_error(y_test, stacked_preds)\n",
    "r2 = r2_score(y_test, stacked_preds)\n",
    "\n",
    "# Print results\n",
    "print(\"📊 Stacked Model Performance:\")\n",
    "print(f\"✅ RMSE: {rmse:.4f}\")\n",
    "print(f\"✅ MAE: {mae:.4f}\")\n",
    "print(f\"✅ R² Score: {r2:.4f}\")\n",
    "\n",
    "# Compare with previous models\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"LSTM\", \"XGBoost\", \"Hybrid (Averaging)\", \"Stacked Model\"],\n",
    "    \"RMSE\": [0.0246, 0.0029, 0.0125, rmse],\n",
    "    \"MAE\": [0.0204, 0.0021, 0.0104, mae],\n",
    "    \"R² Score\": [0.9952, 0.9999, 0.9988, r2]\n",
    "})\n",
    "\n",
    "print(\"\\n🔍 Model Comparison:\")\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGkNX8jVlx42"
   },
   "source": [
    "##### The model comparison highlights that Stacked and XGBoost models outperform others in predicting Netflix stock prices. XGBoost and the Stacked Model achieve the lowest RMSE (0.0029) and MAE (0.0021), demonstrating their high precision. The LSTM model, despite capturing temporal dependencies, has a higher RMSE (0.0246), suggesting limitations in short-term accuracy. The Hybrid (Averaging) model balances predictions but is outperformed by XGBoost and Stacking. The Stacked Model slightly improves over XGBoost (RMSE: 0.002887), indicating that combining multiple models enhances robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "EQP9-9w5P8aP",
    "outputId": "90ff3eba-5f95-4467-a2da-449173ea3add"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Model performance metrics\n",
    "models = [\"LSTM\", \"XGBoost\", \"Hybrid (Averaging)\", \"Stacked Model\"]\n",
    "rmse_values = [0.0246, 0.0029, 0.0125, 0.0029]\n",
    "mae_values = [0.0204, 0.0021, 0.0104, 0.0021]\n",
    "r2_values = [0.9952, 0.9999, 0.9988, 0.9999]\n",
    "\n",
    "# Convert data to DataFrame for visualization\n",
    "df = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"RMSE\": rmse_values,\n",
    "    \"MAE\": mae_values,\n",
    "    \"R² Score\": r2_values\n",
    "})\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot RMSE comparison\n",
    "sns.barplot(x=\"Model\", y=\"RMSE\", hue=\"Model\", data=df, ax=axes[0], palette=\"Blues_r\", legend=False)\n",
    "axes[0].set_title(\"RMSE Comparison\")\n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "\n",
    "# Plot MAE comparison\n",
    "sns.barplot(x=\"Model\", y=\"MAE\", hue=\"Model\", data=df, ax=axes[1], palette=\"Greens_r\", legend=False)\n",
    "axes[1].set_title(\"MAE Comparison\")\n",
    "axes[1].set_ylabel(\"MAE\")\n",
    "\n",
    "# Plot R² Score comparison\n",
    "sns.barplot(x=\"Model\", y=\"R² Score\", hue=\"Model\", data=df, ax=axes[2], palette=\"Reds_r\", legend=False)\n",
    "axes[2].set_title(\"R² Score Comparison\")\n",
    "axes[2].set_ylabel(\"R² Score\")\n",
    "\n",
    "# Rotate x-axis labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=20, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkupqjl4mfEe"
   },
   "source": [
    "##### The provided image visually contrasts the performance of four models—LSTM, XGBoost, Hybrid Averaging, and Stacked Model—across RMSE, MAE, and R² Score metrics. XGBoost consistently demonstrates superior accuracy, exhibiting the lowest error rates (RMSE and MAE) and the highest R² score, indicating the best fit to the data. LSTM, conversely, displays the highest error rates and the lowest, though still high, R² score, suggesting the poorest performance among the group. Both Hybrid Averaging and Stacked Models offer improved results over LSTM, highlighting the efficacy of ensemble methods, with the Stacked Model generally outperforming the Hybrid, likely due to its utilization of a meta-learner. Notably, all models achieve high R² scores, implying reliable predictions, yet XGBoost stands out as the most precise and accurate for this particular task.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036a8f21f1294b1b8217be896dae4ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5fffef203ec547cbb5024742d6c2945b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "732c43f123e944d79048ff27ffa2a154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6ba956fa0464ba39fc69bd7368edd47",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_036a8f21f1294b1b8217be896dae4ed4",
      "value": 50
     }
    },
    "771ca0bb57ec46ae917ae2a416307af1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aabf0605c7a94058ab4e8378fe9662d0",
       "IPY_MODEL_732c43f123e944d79048ff27ffa2a154",
       "IPY_MODEL_f093b6a156d34a8faace7ba4b38c5990"
      ],
      "layout": "IPY_MODEL_91e76d19fca14a4b8a9a513aad55fd5c"
     }
    },
    "855f7b04e40c41b4858161727dab12ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91e76d19fca14a4b8a9a513aad55fd5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ba956fa0464ba39fc69bd7368edd47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aabf0605c7a94058ab4e8378fe9662d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fffef203ec547cbb5024742d6c2945b",
      "placeholder": "​",
      "style": "IPY_MODEL_855f7b04e40c41b4858161727dab12ff",
      "value": "100%"
     }
    },
    "c30e7463edbd44b8ba4a726e5b699d5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f093b6a156d34a8faace7ba4b38c5990": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c30e7463edbd44b8ba4a726e5b699d5d",
      "placeholder": "​",
      "style": "IPY_MODEL_f9df7d820aaa45638a642f626ad28a2c",
      "value": " 50/50 [01:22&lt;00:00,  1.48s/it]"
     }
    },
    "f9df7d820aaa45638a642f626ad28a2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
