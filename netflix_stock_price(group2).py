# -*- coding: utf-8 -*-
"""Netflix Stock Price(Group2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wyp5Wi0FfKGzqxu28ulm6QCWhH9_0Sk5

# Netflix Stock Price Prediction Using LSTM and Another Model of Choice

Business Problem
Data Types (Object Type for Numeric Columns)
Problem: All columns in the dataset, such as Open, High, Low, Close, Adj Close, and Volume, are of type object. These should likely be numeric (e.g., float or int), but they are stored as strings or mixed types.

Solution: Convert the relevant columns (Open, High, Low, Close, Adj Close, and Volume) to their appropriate numeric data types (float for prices, int for volume). This will allow you to perform mathematical operations, such as calculations and analysis, on the data.

Missing Values
Problem: There are missing values in some columns (High, Low, Close, Adj Close, Volume), as indicated by the non-null counts.

The missing values could be caused by incomplete records or issues during data collection.

Solution: Handle missing values either by filling them (e.g., with forward fill, backward fill, or interpolation) or by removing the rows that have missing values (if appropriate).

Date Column Format
Problem: The Date column is stored as an object (likely a string) instead of a datetime object. This makes it harder to perform date-based filtering, sorting, or any time-based analysis.

Solution: Convert the Date column to a datetime format to enable better time-based operations.

Inconsistent or Out-of-Range Values
Problem: Since stock or financial data is being used (presumably based on column names like Open, Close, etc.), there might be out-of-range values (e.g., negative prices, unrealistic volume numbers). This can happen due to data entry issues or anomalies.

Solution: Conduct a check for values outside expected ranges. For instance:

Check if any Close, Open, High, or Low prices are negative, which should not happen in a financial dataset.

Check Volume for values that are implausibly high or low (e.g., negative or zero volume).

Duplicate Rows
Problem: There may be duplicate rows in the dataset, which could distort analysis or insights.

Solution: Identify and remove duplicate rows.

Date Gaps or Missing Records
Problem: If this is a time-series dataset of stock prices, there might be gaps in the dates (e.g., missing trading days due to weekends, holidays, or data collection issues).

Solution: Verify that the dataset has data for every expected trading day (if it's daily data). You can do this by checking the Date column for any missing or duplicate dates.

Handling of Adjusted Close
Problem: The Adj Close column might need additional attention since it reflects price adjustments for dividends, stock splits, etc., which could lead to discrepancies if not properly understood.

Solution: If needed, verify that the adjusted close prices are used correctly in your analysis, particularly if the data will be used to analyze stock performance over time.

## Objectives

Data cleaning: Convert data types, handle missing values, and remove duplicates.

Exploratory Data Analysis: Understand the data structure, trends, and relationships.

Time Series Analysis: Explore and analyze trends, seasonality, and stationarity in stock prices and volumes.

Anomaly Detection: Identify unusual price movements or trading volumes.

Data Imputation: Address missing data using appropriate imputation techniques.

Stock Price Forecasting: Build and evaluate models to predict future stock prices.

Volume Analysis: Explore the relationship between stock prices and trading volumes.

Stock Split and Dividend Adjustments: Investigate adjustments to Adj Close for stock splits and dividends.

Market Behavior Analysis: Analyze market signals using technical indicators.

Report Generation: Create visualizations and summaries to communicate findings effectively

# **PART 1: DATA PREPROCESSING**
"""

#Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.layers import Input
import keras_tuner as kt
from tensorflow.keras.optimizers import Adam
import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split


rng = np.random.default_rng(seed=42)

"""#Load and explore the dataset.


"""

# Loading the Netflix Stock Prices dataset
df = pd.read_csv('Netflix Inc. (NFLX) Stock Price 2002-2025.csv')

#Checking the data size
df.info()

#Displaying the first 5 rows of the dataset
df.head()

#Displaying the Number of Columns in the dataset
df.columns

#Checking if there is a Missing Value in the Dataset
df.isnull().sum()

"""# Handle missing values and perform data cleaning."""

# Remove trailing spaces from column names
df.columns = df.columns.str.strip()
#Convert the relevant columns to numeric types
columns_to_convert = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
df[columns_to_convert] = df[columns_to_convert].replace({',': ''}, regex=True)
df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')
df.fillna(method='ffill', inplace=True)

#convert the 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

#Verify the data types after conversion
print(df.dtypes)
print(df.head())

#Reinspecting the shape of the data frame after filling in the missing values
df.shape

#Reinspecting the missing values after cleaning the dataframe

df.isnull().sum()

#Generates boxplots for column to visuliaze outliers
def check_outliers_boxplot(df, columns):
    for col in columns:
        plt.figure(figsize=(8, 6))
        sns.boxplot(x=df[col])
        plt.title(f'Box Plot of {col}')
        plt.show()
check_outliers_boxplot(df,['Open','Close','Adj Close','Volume','High','Low'])

# Calculate Q1, Q3, and IQR
Q1 = df['Volume'].quantile(0.25)
Q3 = df['Volume'].quantile(0.75)
IQR = Q3 - Q1
# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Filter outliers
df_filtered = df[(df['Volume'] >= lower_bound) & (df['Volume'] <= upper_bound)]
print(f"Original DataFrame shape: {df.shape}")

print(f"DataFrame shape after removing outliers: {df_filtered.shape}")

#Visualize Stock Price
plt.figure(figsize=(10, 6))
plt.plot(df['Date'], df['Close'], label='Closing Price')
plt.title('Netflix Stock Price History')
plt.xlabel('Date')
plt.ylabel('Price (Ksh)')
plt.legend()
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

#Visualize Volume Analyis
plt.figure(figsize=(10, 6))
plt.bar(df['Date'], df['Volume'], color='blue', alpha=0.7)
plt.title('Trading Volume Over Time')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

""" # Performing feature scaling and normalization where necessary."""

#Scales and normalizes numeric columns in a DataFram
cols_to_scale = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']

#mimax_scaler for LSTM
minmax_scaler =MinMaxScaler(feature_range=(0,1))
df_lstm = df.copy()
df_lstm[cols_to_scale] = minmax_scaler.fit_transform(df_lstm[cols_to_scale])

#Standard Scaling (for XGBoost)
std_scaler = StandardScaler()
df_xgb = df.copy()
df_xgb[cols_to_scale] = std_scaler.fit_transform(df_xgb[cols_to_scale])

# Check the scaled data
print("LSTM Scaled Data Sample:")
print(df_lstm.head())

print("\nXGBoost Scaled Data Sample:")
print(df_xgb.head())

"""#PART 2 : FEATURE ENGINEERING"""

def create_features(df):
    #Measure daily volatility and percentage change
    df['Price_Range'] = df['High']-df['Low']
    df['Daily_Return']=df['Close'].pct_change()

    #Moving Average:Identify short &long term trends
    df['MA_5']=df['Close'].rolling(window=5).mean()
    df['MA_10']=df['Close'].rolling(window=10).mean()
    df['MA_20']=df['Close'].rolling(window=20).mean()

    #Measure price acceleration
    df['Momentum_5']=df['Close'].diff(5)#Price change over 5days
    df['Momentum_10']= df['Close'].diff(10)

    #Measure market uncertanity using rolling std
    df['Volatility_5'] =df['Close'].rolling(window=5).std()
    df['Volatility_10']=df['Close'].rolling(window=10).std()

    #Identifies overbought and oversold conditions
    delta = df['Close'].diff()
    gain=(delta.where(delta>0,0)).rolling(window=14).mean()
    loss=(delta.where(delta<0,0)).rolling(window=14).mean()
    rs=gain/loss
    df['RSI'] =100-(100/(1+rs))

    #Weights recent data more heavily
    df['EMA_5'] = df['Close'].ewm(span=5,adjust=False).mean()
    df['EMA_10']=df['Close'].ewm(span=10,adjust=False).mean()
    df['EMA_20'] =df['Close'].ewm(span=20,adjust=False).mean()

    #Identifies high and low volatility periods
    df['Bollinger_Mean']=df['Close'].rolling(window=20).mean()
    df['Bollinger_Upper']=df['Bollinger_Mean']+2 * df['Close'].rolling(window=20).std()
    df['Bollinger_Upper']=df['Bollinger_Mean']-2 * df['Close'].rolling(window=20).std()

#drop NA values caused by rolling calculations
    df=df.dropna()
    return df

#Apply feature engineering
df=create_features(df)

# Print dataset shape to check if new features were added
print(f"Dataset after feature engineering: {df.shape}")

# Display the last few rows to inspect the new features
display(df.tail())

"""#Utilize lagged variables for time series forecasting"""

def create_lagged_features(df, columns, n_lags=5):
    for col in columns:
        for lag in range(1, n_lags + 1):
          df = df.copy()
          df.loc[:, f'{col}_Lag_{lag}'] = df[col].shift(lag)

    # Drop the first n_lags rows as they will have NaN values due to shifting
    df = df.dropna()

    return df

# Define columns for which you want to create lagged features
columns_to_lag = ['Close', 'Open', 'High', 'Low', 'Volume']

# Apply the function to create lagged features
df_lagged = create_lagged_features(df, columns=columns_to_lag, n_lags=5)

# Display the shape of the dataset after adding lagged variables
print(f"Dataset shape after adding lagged features: {df_lagged.shape}")

# Display the first few rows to inspect the new features
print(df_lagged.head())

"""#Implement rolling statistics such as moving averages."""

def create_rolling_statistics(df,columns,window_sizes=[5,10,20]):
    for col in columns:
        for window in window_sizes:
            # Calculate moving averages
            df.loc[:,f'{col}_MA_{window}'] = df[col].rolling(window=window).mean()
            # Calculate rolling standard deviation
            df.loc[:,f'{col}_STD_{window}'] = df[col].rolling(window=window).std()

    # Drop NaN values resulting from the rolling operation
    df = df.dropna()

    return df

# Define columns for which you want to create rolling statistics
columns_to_rolling = ['Close', 'Open', 'High', 'Low', 'Volume']

# Apply the function to create rolling statistics (MA and std)
df_rolling = create_rolling_statistics(df, columns=columns_to_rolling, window_sizes=[5, 10, 20])

# Display the shape of the dataset after adding rolling statistics
print(f"Dataset shape after adding rolling statistics: {df_rolling.shape}")

# Display the first few rows to inspect the new features
print(df_rolling.head())

"""#PART 3: HYBRID APPROACH MODEL

# Develop an LSTM model to capture temporal dependencies in stock prices.
"""

# Create sequences
def create_sequences(data, seq_length):
    xs = []
    ys = []
    for i in range(len(data) - seq_length):
        # Use all columns except the first(date)
        x = data[i:i + seq_length, 1:]
        # Predict 'Close'
        y = data[i + seq_length, 0]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

seq_length = 60
X, y = create_sequences(df_lstm[cols_to_scale].values, seq_length)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Build the LSTM Model
model = Sequential([
    Input(shape=(X_train.shape[1], X_train.shape[2])), # Add Input layer
    LSTM(100, return_sequences=True),
    LSTM(100, return_sequences=False),
    Dense(50),
    Dense(1)
])

# Compile and Train the Model
model.compile(optimizer='adam', loss='mse')
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)

# Evaluate the Model
loss = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')

#  Make Predictions
predictions = model.predict(X_test)

# Ensure predictions are reshaped correctly
predictions = predictions.reshape(-1, 1)

# Inverse transform using only the 'Close' feature
predictions = minmax_scaler.inverse_transform(
    np.hstack((predictions, np.zeros((predictions.shape[0], len(cols_to_scale) - 1))))
)[:, 0]

# Inverse transform the actual test labels
y_test_original = minmax_scaler.inverse_transform(
    np.hstack((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], len(cols_to_scale) - 1))))
)[:, 0]

# Visualize Predictions
plt.figure(figsize=(10, 6))
plt.plot(y_test_original, label='Actual Prices')
plt.plot(predictions, label='Predicted Prices')
plt.title('Netflix Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

"""# XGBoost Model for comparison"""

#Number of previous days to use for prediction
lag = 10

# 'Close' price and create lagged features
df_lagged = create_lagged_features(df[['Close']], columns=['Close'], n_lags=lag)

# Split features and target
X_xgb = df_lagged.drop(columns=['Close'])
y_xgb = df_lagged['Close']

# Train-test split
X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=0.2, shuffle=False)

# Convert to DMatrix (optimized for XGBoost)
dtrain = xgb.DMatrix(X_train_xgb, label=y_train_xgb)
dtest = xgb.DMatrix(X_test_xgb, label=y_test_xgb)

# XGBoost parameters
params = {
    'objective': 'reg:squarederror',
    # Root Mean Squared Error
    'eval_metric': 'rmse',
    'learning_rate': 0.05,
    'max_depth': 6,
    'n_estimators': 500,
}

# Train model with early stopping
xgb_model = xgb.train(
    params,
    dtrain,
    #n_estimators
    num_boost_round=500,
    evals=[(dtest, 'test')],
    # added early stopping
    early_stopping_rounds=10,
    verbose_eval=False,
)

# Make predictions
y_pred_xgb = xgb_model.predict(dtest)

# Calculate RMSE and R-squared
rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))
r2_xgb = r2_score(y_test_xgb, y_pred_xgb)

print(f'\nXGBoost RMSE: {rmse_xgb:.4f}')
print(f'\nXGBoost R^2 Score: {r2_xgb:.4f}')

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(y_test_xgb.values, label='Actual Prices')
plt.plot(y_pred_xgb, label='Predicted Prices (XGBoost)')
plt.title('Netflix Stock Price Prediction - XGBoost')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

"""# Combine the models effectively for improved prediction accuracy"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error,r2_score


data = np.sin(np.linspace(0, 100, 1000))

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data.reshape(-1, 1))

# Prepare data for LSTM
def create_dataset(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step), 0])
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

time_step = 10
X, y = create_dataset(data_scaled, time_step)

X = X.reshape(X.shape[0], X.shape[1], 1)  # LSTM input shape [samples, time steps, features]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Define and train the LSTM model
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=False, input_shape=input_shape))
    model.add(Dense(units=1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

lstm_model = create_lstm_model((X_train.shape[1], 1))
lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Predict with LSTM
lstm_predictions = lstm_model.predict(X_test)

# Prepare data for XGBoost (reshape to 2D)
X_reshaped = X.reshape(X.shape[0], X.shape[1])
X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_reshaped, y, test_size=0.2, shuffle=False)

# Train the XGBoost model
xgb_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100, max_depth=3)
xgb_model.fit(X_train_xgb, y_train_xgb)

# Predict with XGBoost
xgb_predictions = xgb_model.predict(X_test_xgb)

# Rescale the predictions back to the original scale
lstm_predictions_rescaled = scaler.inverse_transform(lstm_predictions)
xgb_predictions_rescaled = scaler.inverse_transform(xgb_predictions.reshape(-1, 1))

# Evaluate the RMSE for each model
lstm_rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test.reshape(-1, 1)), lstm_predictions_rescaled))
xgb_rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test_xgb.reshape(-1, 1)), xgb_predictions_rescaled))

print(f"LSTM RMSE: {lstm_rmse}")
print(f"XGBoost RMSE: {xgb_rmse}")

# 1. **Weighted Average Ensemble**
# We assign more weight to XGBoost because it performed better
weight_lstm = 0.3
weight_xgb = 0.7

# Combine predictions
ensemble_predictions = weight_lstm * lstm_predictions_rescaled + weight_xgb * xgb_predictions_rescaled

# Evaluate ensemble
ensemble_rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test.reshape(-1, 1)), ensemble_predictions))
print(f"Ensemble RMSE (Weighted Average): {ensemble_rmse}")

# 2. **Stacking (Using Linear Regression as meta-model)**
# Prepare the stacking dataset (use both LSTM and XGBoost predictions as features)
stacking_X = np.column_stack((lstm_predictions_rescaled.flatten(), xgb_predictions_rescaled.flatten()))
stacking_y = scaler.inverse_transform(y_test.reshape(-1, 1))

# Train the meta-model (Linear Regression)
meta_model = LinearRegression()
meta_model.fit(stacking_X, stacking_y)

# Predict with meta-model
stacking_predictions = meta_model.predict(stacking_X)

# Evaluate stacking
stacking_rmse = np.sqrt(mean_squared_error(stacking_y, stacking_predictions))
print(f"Stacking Model RMSE: {stacking_rmse}")

# Plot the comparison
plt.figure(figsize=(12, 6))
plt.plot(scaler.inverse_transform(y_test.reshape(-1, 1)), color='blue', label='Actual')
plt.plot(lstm_predictions_rescaled, color='red', label='LSTM Predictions')
plt.plot(xgb_predictions_rescaled, color='green', label='XGBoost Predictions')
plt.plot(ensemble_predictions, color='purple', label='Ensemble Predictions')
plt.plot(stacking_predictions, color='orange', label='Stacking Model Predictions')
plt.title('Model Comparison: LSTM, XGBoost, Ensemble, Stacking')
plt.legend()
plt.show()

"""#PART 4: HYPERPARAMETER OPTIMIZATION

#LSTM Hyperparameter Optmization and Tuning
"""

# Define model building function
def build_lstm_model(hp):
    model = Sequential()
    model.add(LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50),
                   return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(LSTM(units=hp.Int('units_2', min_value=50, max_value=200, step=50), return_sequences=False))
    model.add(Dense(50))
    model.add(Dense(1))

    # Tune learning rate
    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    return model

# Hyperparameter tuner
tuner = kt.RandomSearch(
    build_lstm_model,
    objective='val_loss',
    max_trials=10,
    executions_per_trial=1,
    directory='lstm_tuning',
    project_name='lstm_optimization'
)

# Run the tuner
tuner.search(X_train, y_train, epochs=10, validation_split=0.1, batch_size=32)

# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best LSTM units: {best_hps.get('units')}, Learning rate: {best_hps.get('learning_rate')}")

# Build final LSTM model
final_lstm_model = Sequential([
    LSTM(200, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(200, return_sequences=False),
    Dense(50),
    Dense(1)
])

# Compile with best learning rate
final_lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# Train the final LSTM model
final_lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)

"""

*    XGBoost Hyperparameter Optmization and Tuning


"""

from sklearn.model_selection import GridSearchCV

# Define parameter grid for tuning
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.001, 0.01, 0.05, 0.1],
    'n_estimators': [100, 300, 500]
}

# Create XGBoost regressor
xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror')

# Use GridSearchCV to find best parameters
grid_search = GridSearchCV(xgb_regressor, param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)
# Reshape X_train to be 2-dimensional
X_train_reshaped = X_train.reshape(X_train.shape[0], -1)
# Fit with reshaped data
grid_search.fit(X_train_reshaped, y_train)
# Best parameters
best_xgb_params = grid_search.best_params_
print("Best XGBoost Parameters:", best_xgb_params)


# Train XGBoost with best parameters
best_xgb_model = xgb.XGBRegressor(**best_xgb_params)  # Unpack dictionary
best_xgb_model.fit(X_train_xgb, y_train_xgb)

"""# Train XGBoost with best parameters
best_xgb_model = xgb.XGBRegressor(best_xgb_params)
best_xgb_model.fit(X_train_xgb, y_train_xgb)"""

# Predict on test data
y_pred_xgb = best_xgb_model.predict(X_test_xgb)

# Evaluate performance
rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))
r2_xgb = r2_score(y_test_xgb, y_pred_xgb)

print(f'Optimized XGBoost RMSE: {rmse_xgb:.4f}')
print(f'Optimized XGBoost RÂ² Score: {r2_xgb:.4f}')

"""#  PART 5: MODEL  INTERPRETABILITY  AND EVALUTION"""

import xgboost as xgb
import shap
import xgboost as xgb
import matplotlib.pyplot as plt
import pandas as pd

feature_names = [f"feature_{i}" for i in range(X_test_xgb.shape[1])]
# If you know the actual names, replace with them:
feature_names = ['Close_Lag_1', 'Close_Lag_2', 'Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5',  'Open_Lag_1', 'Open_Lag_2', 'Open_Lag_3', 'Open_Lag_4', 'Open_Lag_5']


X_test_xgb_df = pd.DataFrame(X_test_xgb, columns=feature_names)

# Calculate SHAP values
explainer = shap.TreeExplainer(best_xgb_model)
shap_values = explainer.shap_values(X_test_xgb_df)

# Summary plot of feature importance
shap.summary_plot(shap_values, X_test_xgb_df)

import shap
import numpy as np

# Define a wrapper function for LSTM predictions
def lstm_predict(X):
    # Reshape back to 3D (time_steps, features) before passing to LSTM
    X = X.reshape((X.shape[0], X.shape[1], 1))
    return lstm_model.predict(X).flatten()

# Select a subset of X_train for explanation
X_sample = X_train[:10]

# Reshape X_sample to 2D before passing to KernelExplainer
X_sample_reshaped = X_sample.reshape((X_sample.shape[0], -1))

# Create a SHAP explainer
explainer_lstm = shap.KernelExplainer(lstm_predict, X_sample_reshaped)

# Select a subset of X_test for SHAP values
X_test_for_shap = X_test[:50]

# Reshape X_test to 2D
X_test_for_shap_reshaped = X_test_for_shap.reshape((X_test_for_shap.shape[0], -1))

# Compute SHAP values
shap_values_lstm = explainer_lstm.shap_values(X_test_for_shap_reshaped)

# Define feature names
num_features = X_test_for_shap_reshaped.shape[1]
feature_names = [f'Close_Lag{i+1}' if i < num_features // 2 else f'Open_Lag{i - num_features // 2 + 1}' for i in range(num_features)]

# Summary plot to see feature importance
shap.summary_plot(shap_values_lstm, X_test_for_shap_reshaped, feature_names=feature_names)

"""#Evaluate model performance using metrics such as RMSE, MAE, and R-squared"""

# Function to evaluate model performance
def evaluate_model(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    return rmse, mae, r2

# Evaluate LSTM Model
lstm_pred_test = model.predict(X_test)
lstm_rmse, lstm_mae, lstm_r2 = evaluate_model(y_test, lstm_pred_test)

# Evaluate XGBoost Model
xgb_pred_test = xgb_model.predict(X_test)
xgb_rmse, xgb_mae, xgb_r2 = evaluate_model(y_test, xgb_pred_test)

# Evaluate Hybrid Model
hybrid_pred_test = hybrid_model.predict(X_test)  # Assuming hybrid_model is your hybrid model
hybrid_rmse, hybrid_mae, hybrid_r2 = evaluate_model(y_test, hybrid_pred_test)

# Print results for comparison
print("LSTM Model Evaluation:")
print(f"RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, R^2: {lstm_r2:.4f}\n")

print("XGBoost Model Evaluation:")
print(f"RMSE: {xgb_rmse:.4f}, MAE: {xgb_mae:.4f}, R^2: {xgb_r2:.4f}\n")

print("Hybrid Model Evaluation:")
print(f"RMSE: {hybrid_rmse:.4f}, MAE: {hybrid_mae:.4f}, R^2: {hybrid_r2:.4f}\n")

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Predictions for LSTM
y_pred_lstm = lstm_model.predict(X_test)  # Ensure predictions are obtained
y_pred_lstm = y_pred_lstm.flatten()  # Flatten if necessary

# Predictions for XGBoost
y_pred_xgb = best_xgb_model.predict(X_test_xgb)

# Calculate evaluation metrics
def evaluate_model(y_true, y_pred, model_name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"ðŸ“Š {model_name} Performance:")
    print(f"âœ… RMSE: {rmse:.4f}")
    print(f"âœ… MAE: {mae:.4f}")
    print(f"âœ… RÂ² Score: {r2:.4f}\n")

# Evaluate LSTM
evaluate_model(y_test, y_pred_lstm, "LSTM")

# Evaluate XGBoost
evaluate_model(y_test_xgb, y_pred_xgb, "XGBoost")

"""#Compare the results of the hybrid model against standalone models"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Get predictions from individual models
lstm_preds = lstm_model.predict(X_test)
xgb_preds = best_xgb_model.predict(X_test_xgb)

# Reshape LSTM predictions to 1D array
lstm_preds = lstm_preds.flatten()

# Define weights for LSTM and XGBoost (adjustable)
w_lstm = 0.5
w_xgb = 0.5

# Compute Hybrid Model Predictions
hybrid_preds = (w_lstm * lstm_preds) + (w_xgb * xgb_preds)

# Compute Metrics
def evaluate_model(y_true, y_pred, model_name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"\nðŸ“Š {model_name} Performance:")
    print(f"âœ… RMSE: {rmse:.4f}")
    print(f"âœ… MAE: {mae:.4f}")
    print(f"âœ… RÂ² Score: {r2:.4f}")
    return rmse, mae, r2

# Evaluate all models
rmse_lstm, mae_lstm, r2_lstm = evaluate_model(y_test, lstm_preds, "LSTM")
rmse_xgb, mae_xgb, r2_xgb = evaluate_model(y_test_xgb, xgb_preds, "XGBoost")
rmse_hybrid, mae_hybrid, r2_hybrid = evaluate_model(y_test, hybrid_preds, "Hybrid Model") # Use y_test for hybrid

# Compare Results
import pandas as pd
comparison_df = pd.DataFrame({
    "Model": ["LSTM", "XGBoost", "Hybrid"],
    "RMSE": [rmse_lstm, rmse_xgb, rmse_hybrid],
    "MAE": [mae_lstm, mae_xgb, mae_hybrid],
    "RÂ² Score": [r2_lstm, r2_xgb, r2_hybrid]
})

print("\nðŸ” Model Comparison:")
print(comparison_df)

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Step 1: Get predictions from LSTM and XGBoost
lstm_train_preds = lstm_model.predict(X_train)
lstm_test_preds = lstm_model.predict(X_test)

xgb_train_preds = best_xgb_model.predict(X_train_xgb)
xgb_test_preds = best_xgb_model.predict(X_test_xgb)

# Step 2: Stack predictions as new features
stacked_train = np.column_stack((lstm_train_preds, xgb_train_preds))
stacked_test = np.column_stack((lstm_test_preds, xgb_test_preds))

# Step 3: Train a meta-model (Linear Regression)
meta_model = LinearRegression()
meta_model.fit(stacked_train, y_train)  # Train using true labels

# Step 4: Get final stacked predictions
stacked_preds = meta_model.predict(stacked_test)

# Step 5: Evaluate stacked model
rmse = np.sqrt(mean_squared_error(y_test, stacked_preds))
mae = mean_absolute_error(y_test, stacked_preds)
r2 = r2_score(y_test, stacked_preds)

# Print results
print("ðŸ“Š Stacked Model Performance:")
print(f"âœ… RMSE: {rmse:.4f}")
print(f"âœ… MAE: {mae:.4f}")
print(f"âœ… RÂ² Score: {r2:.4f}")

# Compare with previous models
comparison_df = pd.DataFrame({
    "Model": ["LSTM", "XGBoost", "Hybrid (Averaging)", "Stacked Model"],
    "RMSE": [0.0246, 0.0029, 0.0125, rmse],
    "MAE": [0.0204, 0.0021, 0.0104, mae],
    "RÂ² Score": [0.9952, 0.9999, 0.9988, r2]
})

print("\nðŸ” Model Comparison:")
print(comparison_df)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Model performance metrics
models = ["LSTM", "XGBoost", "Hybrid (Averaging)", "Stacked Model"]
rmse_values = [0.0246, 0.0029, 0.0125, 0.0029]
mae_values = [0.0204, 0.0021, 0.0104, 0.0021]
r2_values = [0.9952, 0.9999, 0.9988, 0.9999]

# Convert data to DataFrame for visualization
df = pd.DataFrame({
    "Model": models,
    "RMSE": rmse_values,
    "MAE": mae_values,
    "RÂ² Score": r2_values
})

# Set style
sns.set(style="whitegrid")

# Create a figure with subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Plot RMSE comparison
sns.barplot(x="Model", y="RMSE", hue="Model", data=df, ax=axes[0], palette="Blues_r", legend=False)
axes[0].set_title("RMSE Comparison")
axes[0].set_ylabel("RMSE")

# Plot MAE comparison
sns.barplot(x="Model", y="MAE", hue="Model", data=df, ax=axes[1], palette="Greens_r", legend=False)
axes[1].set_title("MAE Comparison")
axes[1].set_ylabel("MAE")

# Plot RÂ² Score comparison
sns.barplot(x="Model", y="RÂ² Score", hue="Model", data=df, ax=axes[2], palette="Reds_r", legend=False)
axes[2].set_title("RÂ² Score Comparison")
axes[2].set_ylabel("RÂ² Score")

# Rotate x-axis labels
for ax in axes:
    ax.set_xticks(range(len(models)))  # Set tick positions
    ax.set_xticklabels(models, rotation=20, ha="right")

plt.tight_layout()
plt.show()